{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbe3826",
   "metadata": {},
   "source": [
    "# [GD15] NLP Frameworkì˜ í™œìš©\n",
    "# 15-1. ë“¤ì–´ê°€ë©°\n",
    "## NLP ê¸°ìˆ ì˜ ë°œì „ê³¼ framework\n",
    "ì§€ë‚œ ì‹œê°„ê¹Œì§€ ìš°ë¦¬ëŠ” ìµœê·¼ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆëŠ” ë‹¤ì–‘í•œ Modern NLPì˜ ë…¼ë¬¸ê³¼ ëª¨ë¸ë“¤ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ëª¨ë“  ëª¨ë¸ë“¤ì„ ì§ì ‘ ì§œë³´ëŠ” ê²ƒì´ ì‹¤ë ¥ í–¥ìƒì— ë„ì›€ì´ ë˜ì§€ë§Œ, ë§¤ë²ˆ ìƒˆë¡œìš´ ë…¼ë¬¸ë“¤ì„ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ëŠ” ì—†ê² ì£ . ì„¤ë ¹ êµ¬í˜„í•´ ë³¸ë‹¤ í•˜ë”ë¼ë„ ê°ê°ì˜ ëª¨ë¸ë“¤ì´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” ì—„ì²­ë‚˜ê²Œ ë§ì€ ì»´í“¨íŒ… ìì›ì„ ë™ì›í•œ pre-training ì‘ì—…ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ, ë…¼ë¬¸ê³¼ í•¨ê»˜ ê³µê°œëœ ë¦¬ì„œì¹˜ ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ì…‹ê³¼ ë¬¸ì œì— ì ìš©í•  ìˆ˜ëŠ” ìˆì§€ë§Œ, ë§¤ë²ˆ í”„ë¡œì íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì½”ë“œ ìŠ¤íƒ€ì¼ê³¼, ë‹¤ë¥¸ framework(tensorflow, pytorch ë“±)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë§ì€ ëª¨ë¸ë“¤ì„ ë¶„ì„í•´ì„œ êµ¬ì¡°ë¥¼ íŒŒì•…í•œ í›„ ì§ì ‘ ëŒë ¤ë³´ê³  ì´ë¥¼ ì ìš©í•˜ê¸°ë€ ì‰½ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ° ê³ ë¯¼ë“¤ì„ ëª¨ë‘ í•œêº¼ë²ˆì— í•´ê²°í•´ ì¤„ ìˆ˜ ìˆëŠ” ê²Œ ë°”ë¡œ NLP Frameworkì…ë‹ˆë‹¤!!\n",
    "\n",
    "ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ frameworkë€ í”„ë¡œì íŠ¸ì˜ ë¼ˆëŒ€ë¥¼ ì´ë£¨ëŠ” í´ë˜ìŠ¤ì™€ ì¸í„°í˜ì´ìŠ¤ì˜ ì§‘í•©ì„ ë§í•©ë‹ˆë‹¤. í•´ë‹¹ ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°˜ì˜í•˜ì—¬ í™•ì¥ ê°€ëŠ¥í•œ í…œí”Œë¦¿ í˜•íƒœë¡œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì—, frameworkë¥¼ ì´ìš©í•´ ì†ì‰½ê²Œ ë‹¤ì–‘í•œ ì‘ìš© í”„ë¡œê·¸ë¨ì„ ì œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœê·¼ NLP ë¶„ì•¼ì—ì„  transformer ê¸°ë°˜ì˜ BERT ë“± ë‹¤ì–‘í•œ pretrained modelì´ ë°œí‘œë˜ê³ , ì´ë¥¼ í™œìš©í•œ ì „ì´í•™ìŠµ(transfer learning)ì„ í†µí•´ ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ë¥¼ ì†ì‰½ê²Œ êµ¬í˜„í•˜ëŠ” íë¦„ì´ ë‘ë“œëŸ¬ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° NLP ë¶„ì•¼ì˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ NLP ë¶„ì•¼ì˜ frameworkê°€ ì†ì† ë°œí‘œë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ NLP frameworkë“¤ì€ NLP ë¶„ì•¼ì˜ ìµœì‹  ë…¼ë¬¸ë“¤ì˜ ë¦¬ì„œì¹˜ ì½”ë“œë¥¼ ë¯¸ë¦¬ êµ¬í˜„í•˜ì—¬ pretrained modelì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ frameworkì˜ ì‚¬ìš©ìê°€ ì•„ì£¼ ì†ì‰½ê²Œ ì´ë¥¼ ê°€ì ¸ë‹¤ê°€ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ë§ê²Œ finetuningí•˜ê±°ë‚˜ í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ NLP frameworkë“¤ì€ íƒœìŠ¤í¬ë‚˜ ë°ì´í„°ì…‹, ëª¨ë¸ì— ë¬´ê´€í•˜ê²Œ í†µì¼ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ëœ í´ë˜ìŠ¤ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ, ìµœì†Œí•œì˜ ì½”ë“œ êµ¬í˜„ë§Œìœ¼ë¡œë„ ë‹¤ì–‘í•œ ë³€í™”ì— ëŒ€ì‘í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì •ë§ ë‹¤ì–‘í•œ frameworkë“¤ì´ ì¡´ì¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë©‹ì§„ NLP frameworkë“¤ì´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì‚´í´ë³´ê² ì§€ë§Œ, ì´ë²ˆ ì‹œê°„ì— ìš°ë¦¬ëŠ” íŠ¹íˆ Huggingface(ğŸ¤—)ì˜ transformersë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ NLPì˜ ëª¨ë¸ë“¤ì„ ë‹¤ë£¨ëŠ” ë°©ë²•ì— ëŒ€í•´ ì´ì•¼ê¸° ë‚˜ëˆ ë³¼ê¹Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì˜¤ëŠ˜ì˜ ëª©ì°¨\n",
    "1. ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„\n",
    "2. Huggingface transformers ê°œìš”\n",
    "3. Huggingface transformers (1) Model\n",
    "4. Huggingface transformers (2) Tokenizer\n",
    "5. Huggingface transformers (3) Processor\n",
    "6. Huggingface transformers (4) Config\n",
    "7. Huggingface transformers (5) Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430d305",
   "metadata": {},
   "source": [
    "# 15-2. ë‹¤ì–‘í•œ NLP Frameworkì˜ ì¶œí˜„\n",
    "ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ì£¼ë¡œ Huggingface transformers Frameworkë¥¼ í†µí•´ NLP framworkê°€ ì–´ë–¤ ê²ƒì¸ì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Huggingface transformersì´ì „ì—ë„ NLP ë¶„ì•¼ì— ë„ë¦¬ ì•Œë ¤ì§„ frameworkë“¤ì´ ë§ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆë„ ë„ë¦¬ í™œìš©ë˜ê³  ìˆëŠ” ê²ƒë“¤ ìœ„ì£¼ë¡œ ì¢…ë¥˜ë³„ë¡œ ëª‡ ê°€ì§€ë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "(ì°¸ê³ ) ì´ë²ˆ ìŠ¤í…ì˜ ë‚´ìš©ì€ [Popular NLP Libraries of 2022](https://medium.com/nlplanet/awesome-nlp-21-popular-nlp-libraries-of-2022-2e07a914248b)ì— ì •ë¦¬ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŒì„ ë°í™ë‹ˆë‹¤.\n",
    "\n",
    "## General Framework for NLP\n",
    "ì—¬ê¸°ì„œ ì†Œê°œí•  frameworkë“¤ì€ NLP ë¬¸ì œë¥¼ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆëŠ” í†µí•©ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ ëª©í‘œë¡œ ì„¤ê³„ëœ ê²ƒë“¤ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œëŠ” AllenNLP, Fairseq, Fast.aiê°€ ìˆìœ¼ë©°, Googleì˜ tensor2tensor í”„ë¡œì íŠ¸ë„ ê°™ì€ ë²”ì£¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”¶ AllenNLP\n",
    "- ì œê³µì : Allen AI Institute\n",
    "- Website : https://allennlp.org/\n",
    "- Github : https://github.com/allenai/allennlp\n",
    "- Backend : PyTorch\n",
    "2018ë…„ ì´ˆë°˜ì— Contextual Word Embeddingì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ì¸ ELMOë¥¼ ë°œí‘œí•˜ë©´ì„œ ìœ ëª…í•´ì§„ Allen Instituteì—ì„œ ë§Œë“  NLP frameworkì…ë‹ˆë‹¤. ë‹¹ì‹œ ELMOëŠ” [GLUE Benchmark Test](https://gluebenchmark.com/)ì™€ ê°™ì´ 10ê°€ì§€ë‚˜ ë˜ëŠ” ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ì˜ ëª¨ë¸ì„ finetuneí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ê¸°ì¡´ì˜ State-of-the-art ê¸°ë¡ì„ ê²½ì‹ í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¬ê¸°ì— í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì†ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìœ ì—°í•œ í”„ë¡œì íŠ¸ë¥¼ êµ¬ì„±í•´ì•¼ í–ˆê³ , ì´ë¥¼ í™•ì¥í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ NLP frameworkë¡œ ë°œì „í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´í›„ AllenNLPëŠ” Glue datasetì˜ baseline í”„ë¡œì íŠ¸ [Starting Baseline](https://github.com/nyu-mll/GLUE-baselines)ë¥¼ ì œê³µí•˜ê¸°ë„ í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/images/GN-9-L-01.max-800x600.png)\n",
    "*[ì¶œì²˜ : AllenNLP Guide(https://guide.allennlp.org/building-your-model#1)]*\n",
    "\n",
    "íƒœìŠ¤í¬ì™€ ëª¨ë¸ì„ ë¶„ë¦¬í•´ì„œ, í•œ ê°€ì§€ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ í•˜ë‚˜ì˜ íƒœìŠ¤í¬ë¥¼ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„¤ê³„ëŠ” AllenNLPê°€ ì²˜ìŒ ì‹œë„í•œ ê²ƒì€ ë¬¼ë¡  ì•„ë‹™ë‹ˆë‹¤ë§Œ, ELMOì™€ ê°™ì€ pretrained modelì˜ ì„±ê³µì„ ë°”íƒ•ìœ¼ë¡œ NLP frameworkë¥¼ ì™„ì„±í•´ ë‚˜ê°€ë ¤ëŠ” AllenNLPì˜ ì‹œë„ëŠ” ì´í›„ ë§ì€ ì•„ì´ë””ì–´ë¥¼ ì œê³µí•˜ì˜€ìŠµë‹ˆë‹¤. AllenNLPëŠ” í˜„ì¬ëŠ” ELMO ì´ì™¸ì—ë„ BERT ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì˜ í™œìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¨, AllenNLPëŠ” PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©° ëª¨ë¸ì´ torch.nn.Moduleì„ ìƒì†ë°›ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. Tensorflowë‚˜ Keras ê¸°ë°˜ìœ¼ë¡œ AllenNLPë¥¼ í™œìš©í•˜ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”¶ Fairseq\n",
    "- ì œê³µì : Facebook AI Research\n",
    "- Website : https://fairseq.readthedocs.io/en/latest\n",
    "- Github : https://github.com/pytorch/fairseq\n",
    "- Backend : PyTorch\n",
    "FairseqëŠ” ê¾¸ì¤€íˆ NLP ì—°êµ¬ì„±ê³¼ë¥¼ ë‚´ê³  ìˆëŠ” Facebook AI Researchì˜ NLP Frameworkì…ë‹ˆë‹¤. ë¹„ë‹¨ ìì—°ì–´ì²˜ë¦¬ì—ë§Œ êµ­í•œëœ ê²ƒì´ ì•„ë‹ˆë¼ ì´ë¦„ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ CNN, LSTM ë“± ì „í†µì ì¸ ëª¨ë¸ë¡œë¶€í„°, ìŒì„±ì¸ì‹/í•©ì„± ë“± sequentialí•œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë¶„ì•¼ë¥¼ ë‘ë£¨ ë‹¤ë£¨ëŠ” ë‹¤ì–‘í•œ pretrained modelì„ í•¨ê»˜ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—­ì‹œ Facebookì˜ frameworkë‹µê²Œ PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”¶ Fast.ai\n",
    "ì œê³µì : fast.ai\n",
    "Website : http://docs.fast.ai/\n",
    "Github : https://github.com/fastai/fastai\n",
    "Backend : PyTorch\n",
    "fast.aiëŠ” ì´ë¦„ì— ê±¸ë§ê²Œ, ë¹ ë¥´ê²Œ ë°°ìš°ê³  ì‰½ê²Œ ëª¨ë¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ì´ë ˆë²¨ APIì™€ Application ë¸”ë¡ê¹Œì§€ ì†ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¹„ë‹¨ NLP ë¶„ì•¼ ë¿ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ ë¶„ì•¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì—­ì‹œ PyTorch ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![content img](https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-9-L-02.png)\n",
    "*[ì¶œì²˜ : fast.ai (https://github.com/fastai/fastai)]*\n",
    "\n",
    "### ğŸ”¶ tensor2tensor\n",
    "- ì œê³µì : Google Brain\n",
    "- Github : https://github.com/tensorflow/tensor2tensor (deprecated)\n",
    "- New Github : https://github.com/google/trax\n",
    "- Backend : Tensorflow\n",
    "Google Brainì—ì„œ 2017ë…„ì— transformer ë…¼ë¬¸ì„ ë°œí‘œí•˜ë©´ì„œ ê·¸ êµ¬í˜„ì²´ë¡œ í•¨ê»˜ ê³µìœ í–ˆë˜ í”„ë¡œì íŠ¸ê°€ ë°”ë¡œ tensor2tensorì˜€ìŠµë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ ì—­ì‹œ 'Attention is all you need'ë¼ëŠ” ë…¼ë¬¸ì˜ ì œëª©ì²˜ëŸ¼, transformerë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì™€ ë‹¤ì–‘í•œ ëª¨ë¸ì„ í•˜ë‚˜ì˜ frameworkì— í†µí•©í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í•˜ì˜€ìŠµë‹ˆë‹¤. ì´í›„ 2019ë…„ë¶€í„° Googleì€ Tensorflow V2 ê¸°ë°˜ìœ¼ë¡œ pretrained modelì˜ ì§€ì›ì„ ê°•í™”í•œ traxë¼ëŠ” í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ë©´ì„œ, 2020ë…„ë„ë¶€í„°ëŠ” tensor2tensorì˜ ê°œë°œì„ ì¤‘ë‹¨í•˜ê³  ê´€ë ¨ ê¸°ëŠ¥ì„ traxë¡œ í†µí•©ì´ê´€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "Tensorflow ê¸°ë°˜ì˜ NLP frameworkì´ ìƒëŒ€ì ìœ¼ë¡œ ë“œë¬¸ ê°€ìš´ë°, Tensorflow ê¸°ë°˜ì˜ NLP ì—°êµ¬ê°œë°œì„ ì§„í–‰í•œë‹¤ë©´ ì£¼ëª©í•´ ë³¼ ë§Œí•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "## Preprocessing Libraries\n",
    "ì•„ë˜ëŠ” ì „í†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë˜ NLP ë¶„ì•¼ì˜ ì „ì²˜ë¦¬ ê´€ë ¨ frameworkë“¤ì…ë‹ˆë‹¤. ìœ„ì—ì„œ ì†Œê°œí•œ frameworkë“¤ì²˜ëŸ¼ ì „ì²˜ë¦¬-ëª¨ë¸ë§-íƒœìŠ¤í¬ í›ˆë ¨/í‰ê°€ë¥¼ í†µí•©ì ìœ¼ë¡œ ì„¤ê³„í•˜ì—¬ NLP íƒœìŠ¤í¬ë¥¼ ì œë„ˆëŸ´í•˜ê²Œ ìˆ˜í–‰í•˜ê²Œ ì„¤ê³„í•œ ê²ƒì´ ì•„ë‹ˆë¼ tokenization, tagging, parsing ë“± íŠ¹ì • ì „ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•´ ì„¤ê³„ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ê°€ê¹ìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì•„ë˜ ì˜ˆì‹œë¡œ ë“  Spacy, NLTK, TorchText ë“±ì´ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ì¸ ê²½ìš° KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ë™ì¼í•œ ì—­í• ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”¶ Spacy\n",
    "- Website : https://spacy.io/\n",
    "- Github : https://github.com/explosion/spaCy\n",
    "\n",
    "### ğŸ”¶ NLTK\n",
    "- Website : https://www.nltk.org/\n",
    "- Github : https://github.com/nltk/nltk\n",
    "\n",
    "### ğŸ”¶ TorchText\n",
    "- Website : https://torchtext.readthedocs.io/en/latest/\n",
    "- Github : https://github.com/pytorch/text\n",
    "\n",
    "### ğŸ”¶ KoNLPy\n",
    "- Website : https://konlpy.org/en/latest/\n",
    "- Github : https://github.com/konlpy/konlpy\n",
    "- Transformer-based Framework\n",
    "\n",
    "### ğŸ”¶ Huggingface transformers\n",
    "- ì œê³µì : Huggingface.co\n",
    "- Website : https://huggingface.co/transformers/\n",
    "- Github : https://github.com/huggingface/transformers\n",
    "- Backend : PyTorch and Tensorflow\n",
    "ì—¬ê¸°ì—ëŠ” í˜„ì¬ ê°€ì¥ ì£¼ëª©ë°›ê³  ìˆëŠ” NLP Frameworkì¸ Huggingface transformersê°€ ìˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤ Huggingfaceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ìµœê·¼ ëª¨ìŠµì€ ì´ë¯¸ ì•„ì£¼ generalí•œ NLP frameworkì˜ ëª¨ìŠµì„ ì¶©ë¶„íˆ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ˆê¸°ì—ëŠ” BERT ë“± ë‹¤ì–‘í•œ transformer ê¸°ë°˜ì˜ pretrained modelì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ PyTorch ê¸°ë°˜ì˜ wrapper í˜•íƒœë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì „í†µì ì¸ ëª¨ë¸ê¹Œì§€ í¬ê´„í•˜ë ¤ê³  í–ˆë˜ ì´ì „ì˜ general NLP Framework ë“¤ì— ë¹„í•´, Huggingfaceì˜ transformersëŠ” pretrained model í™œìš©ì„ ì£¼ë¡œ ì§€ì›í•˜ë©°, tokenizer ë“± ì „ì²˜ë¦¬ ë¶€ë¶„ë„ pretrained modelë“¤ì´ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” Subword tokenizer ê¸°ë²•ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„ ë‹¤ìŒ ìŠ¤í…ë¶€í„°ëŠ” Huggingfaceì˜ transformersì— ëŒ€í•´ ì§‘ì¤‘ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec301e",
   "metadata": {},
   "source": [
    "# 15-3. Huggingface transformers ê°œìš”\n",
    "\n",
    "## Why Huggingface?\n",
    "### (1) ê´‘ë²”ìœ„í•˜ê³  ì‹ ì†í•œ NLP ëª¨ë¸ ì§€ì›\n",
    "HuggingfaceëŠ” ë§ì€ ì‚¬ëŒë“¤ì´ ìµœì‹  NLP ëª¨ë¸ë“¤ì„ ë”ìš± ì†ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ë§Œë“¤ê¸° ì‹œì‘í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê·¸ëŸ°ì§€ ìƒˆë¡œìš´ ë…¼ë¬¸ë“¤ì´ ë°œí‘œë  ë•Œë§ˆë‹¤, ë³¸ì¸ë“¤ì˜ frameworkì— í¡ìˆ˜ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, pretrained modelì„ ì œê³µí•˜ê³ , datasetê³¼ tokenizerë¥¼ ë”ìš± ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ frameworkí™”ì‹œí‚¤ê³  ìˆëŠ” í–‰ë³´ë„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ frameworkë“¤ë„ ì´ëŸ° ì‘ì—…ì„ í•˜ì§€ ì•ŠëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, Huggingfaceì˜ ì§€ì› ë²”ìœ„ê°€ ê°€ì¥ ê´‘ë²”ìœ„í•˜ê³ , ìµœì‹  ë…¼ë¬¸ì„ ì§€ì›í•˜ëŠ” ì†ë„ë„ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "### (2) PyTorchì™€ Tensorflow ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n",
    "transformersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ PyTorchë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì ¸ìˆìŠµë‹ˆë‹¤. ë§ì€ utilityê°€ PyTorch ìœ„ì£¼ë¡œ ì‘ì„±ì´ ë˜ì–´ìˆê¸´ í•˜ì§€ë§Œ, ìµœê·¼ì—ëŠ” Tensorflowë¡œë„ í•™ìŠµí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë” ê³„ì†í•´ì„œ frameworkë¥¼ í™•ì¥í•˜ê³  ìˆëŠ” ì¤‘ì…ë‹ˆë‹¤. ì´ë ‡ë“¯ Huggingface transformersë¥¼ ë°”íƒ•ìœ¼ë¡œ Tensorflowì™€ PyTorchë¼ëŠ” Backendì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´ ì–´ë–¤ í™˜ê²½ì—ë“  ì‰½ê²Œ ì ìš© ê°€ëŠ¥í•œ í‘œì¤€ frameworkì˜ ì§€ìœ„ë¥¼ ë‹¤ì ¸ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### (3) ì˜ ì„¤ê³„ëœ framework êµ¬ì¡°\n",
    "HuggingFaceì˜ ëª©í‘œì²˜ëŸ¼ ì´ frameworkëŠ” ì‰½ê³  ë¹ ë¥´ê²Œ ì–´ë– í•œ í™˜ê²½ì—ì„œë„ NLPëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëŠì„ì—†ì´ ë³€í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ë˜í•œ ì‚¬ìš©í•˜ê¸° ì‰½ê³  ì§ê´€ì ì¼ë¿ë”ëŸ¬ ëª¨ë¸ì´ë‚˜ íƒœìŠ¤í¬, ë°ì´í„°ì…‹ì´ ë‹¬ë¼ì§€ë”ë¼ë„ ë™ì¼í•œ í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì˜ ì¶”ìƒí™”ë˜ê³  ëª¨ë“ˆí™”ëœ API ì„¤ê³„ê°€ ìˆê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì‹œì‘í•˜ê¸°\n",
    "ì—­ì‹œ ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ê²ƒì€ í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ì£ ! tensorflowë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ ì²˜ìŒ ì„¤ì¹˜í–ˆë˜ ê²ƒì²˜ëŸ¼, huggingfaceì˜ Transformersë„ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (í´ë¼ìš°ë“œ ì‚¬ìš©ìë¶„ë“¤ì€ ì´ë¯¸ ì„¤ì¹˜ê°€ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ëˆˆìœ¼ë¡œ í™•ì¸ë§Œ í•˜ê³  ê°€ì‹œë©´ ë©ë‹ˆë‹¤.)\n",
    "```\n",
    "$ pip install transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ef1132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c41dce2bcd64e4c891e1732af34cc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87facef1a5d142a694b877db4453a8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb598248abc94b41bb4b24402b2ae75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6accf4a1d4a49839901df398bd75584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978194236755371}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4999150",
   "metadata": {},
   "source": [
    "## Huggingface transformers ì„¤ê³„êµ¬ì¡° ê°œìš”\n",
    "frameworkë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” frameworkê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ ê·¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ê² ì£ ??\n",
    "\n",
    "NLP frameworkê°€ NLPëª¨ë¸ì„ í†µí•´ ì–´ë– í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì´ ì–´ë–»ê²Œ ì§„í–‰ë ì§€ ìƒê°í•´ ë´…ì‹œë‹¤.\n",
    "\n",
    "ë¨¼ì €, 1) Taskë¥¼ ì •ì˜í•˜ê³  ê·¸ì— ë§ê²Œ datasetì„ ê°€ê³µì‹œí‚µë‹ˆë‹¤. ê·¸ ì´í›„ 2) ì ë‹¹í•œ modelì„ ì„ íƒí•˜ê³  ì´ë¥¼ ë§Œë“­ë‹ˆë‹¤. 3)modelì— ë°ì´í„°ë“¤ì„ íƒœì›Œì„œ í•™ìŠµì„ ì‹œí‚¤ê³ , ì´ë¥¼ í†µí•´ ë‚˜ì˜¨ 4)weightì™€ ì„¤ì •(config)ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤. ì €ì¥í•œ modelì˜ checkpointëŠ” 5)ë°°í¬í•˜ê±°ë‚˜, evaluationì„ í•  ë•Œ ì‚¬ìš©í•˜ê³ ëŠ” í•˜ì£ .\n",
    "\n",
    "transformersëŠ” ìœ„ì™€ ê°™ì€ íë¦„ì— ë§ì¶”ì–´ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "taskë¥¼ ì •ì˜í•˜ê³  datasetì„ ì•Œë§ê²Œ ê°€ê³µí•˜ëŠ” Processors, í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” Tokenizer, ë‹¤ì–‘í•œ modelì„ ì •ì˜í•œ Model, optimizerì™€ í•™ìŠµ schedule(warm up ë“±)ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” Optimization, í•™ìŠµ ê³¼ì •ì„ ì „ë°˜ì„ ê´€ë¦¬í•˜ëŠ” Trainer, weightì™€ tokenizer, modelì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ ê°ì¢… ì„¤ì •ì„ ì €ì¥í•˜ëŠ” Config ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë‹¤ìŒ ìŠ¤í…ë¶€í„°, Huggingfaceì˜ ê° ë¶€ë¶„ì„ ì´ë£¨ê³  ìˆëŠ” í´ë˜ìŠ¤ êµ¬ì¡°ì— ëŒ€í•´ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc8697",
   "metadata": {},
   "source": [
    "# 15-4. Huggingface transformers (1) Model\n",
    "transformersì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¶€ë¶„ì€ ì•„ë¬´ë˜ë„ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ë“¤ì€ PretrainedModel í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ê³  ìˆìŠµë‹ˆë‹¤. PretrainedModel í´ë˜ìŠ¤ëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , ë‹¤ìš´ë¡œë“œí•˜ê³ , ì €ì¥í•˜ëŠ” ë“± ëª¨ë¸ ì „ë°˜ì— ê±¸ì³ ì ìš©ë˜ëŠ” ë©”ì†Œë“œë“¤ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒì† êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ì‹¤ì œë¡œ ì‚¬ìš©í•  ëª¨ë¸ì´ BERTì´ê±´, GPTì´ê±´ ìƒê´€ì—†ì´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ë‹¤ìš´ë¡œë“œ/ì €ì¥í•˜ëŠ” ë“±ì˜ ì‘ì—…ì— í™œìš©í•˜ëŠ” ë©”ì†Œë“œëŠ” ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê²ƒì„ ë™ì¼í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì²« ë²ˆì§¸ë¡œëŠ” taskì— ì í•©í•œ ëª¨ë¸ì„ ì§ì ‘ ì„ íƒí•˜ì—¬ importí•˜ê³ , ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë¡œë“œí•  ë•ŒëŠ” from_pretrainedë¼ëŠ” ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë©°, Huggingfaceì˜ pretrained ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Huggingfaceì—ì„œ ì œê³µí•˜ëŠ” pretrained ëª¨ë¸ì´ë¼ë©´ ëª¨ë¸ì˜ ì´ë¦„ì„ stringìœ¼ë¡œ, ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë¼ë©´ configì™€ ëª¨ë¸ì„ ì €ì¥í•œ ê²½ë¡œë¥¼ stringìœ¼ë¡œ ë„˜ê²¨ì£¼ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002723f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5683faf2f44e018295f341c286d583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0b6ae254694fe490054f84385bd001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a5909",
   "metadata": {},
   "source": [
    "\n",
    "ë‘ ë²ˆì§¸ ë°©ë²•ì€, AutoModelì„ ì´ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì— ê´€í•œ ì •ë³´ë¥¼ ì²˜ìŒë¶€í„° ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë˜ì–´ ì¡°ê¸ˆ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426482b0",
   "metadata": {},
   "source": [
    "ë°©ê¸ˆ ìœ„ì—ì„œ bert-base-casedë¼ê³  ì–¸ê¸‰ëœ ë¶€ë¶„ì´ ë³´ì´ì‹œë‚˜ìš”? ì´ê²ƒì€ Model IDì…ë‹ˆë‹¤. Huggingfaceê°€ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ pretrained modelì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ ì¤‘ ì–´ëŠ ê²ƒì„ ì„ íƒí• ì§€ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•´ ì´ IDë¥¼ í™œìš©í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì–´ë–¤ ëª¨ë¸ì´ ì§€ì›ë˜ëŠ”ì§€ ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [Pretrained models](https://huggingface.co/transformers/pretrained_models.html)\n",
    "\n",
    "ê·¸ëŸ°ë°, ìœ„ì—ì„œ ì†Œê°œí•œ ë‘ ê°€ì§€ ë°©ë²•ì˜ ì°¨ì´ê°€ íŒŒì•…ë˜ì‹œë‚˜ìš”? ë¶ˆëŸ¬ì˜¤ê³ ì í•˜ëŠ” ëª¨ë¸ì˜ IDëŠ” bert-base-casedë¡œì„œ ë™ì¼í•©ë‹ˆë‹¤. ì‚¬ìš©ë²•ë„ ê±°ì˜ ë™ì¼í•œë°ìš”, ê²°ê³¼ì ìœ¼ë¡œ model.__class__ë¥¼ í™•ì¸í•´ ë³´ë©´ ì•½ê°„ì˜ ì°¨ì´ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ ë™ì¼í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ Pretrain, Downstream Task ë“± ìš©ë„ì— ë”°ë¼ ëª¨ë¸ì˜ Inputì´ë‚˜ Output shapeê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AutoModelì„ í™œìš©í•œë‹¤ë©´ ëª¨ë¸ì˜ ìƒì„¸ì •ë³´ë¥¼ í™•ì¸í•  í•„ìš” ì—†ì´ Model IDë§Œìœ¼ë¡œë„ ì†ì‰½ê²Œ ëª¨ë¸ êµ¬ì„±ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì •í™•í•œ ìš©ë„ì— ë§ê²Œ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ë³„ ìƒì„¸ ì•ˆë‚´ í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì„œ ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ BERTì˜ ìƒì„¸ í˜ì´ì§€ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- [MODELS - BERT](https://huggingface.co/transformers/model_doc/bert.html)\n",
    "\n",
    "ëª¨ë¸ë§ˆë‹¤ ê·¸ êµ¬ì¡°ëŠ” ë‹¤ë¥´ì§€ë§Œ ëŒ€ë¶€ë¶„ í•´ë‹¹ ëª¨ë¸ ì´ë¦„ì„ ê°€ì§„ í´ë˜ìŠ¤(eg. TFBertModel)ê³¼ MainLayer class(eg. TFBertMainLayer)ì™€ Attention Class, Embedding Class ë“±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹µë‹ˆë‹¤. ì¶”í›„ì— ëª¨ë¸ì´ ì–´ë–»ê²Œ ì§œì—¬ì¡ŒëŠ”ì§€ ë³´ì‹¤ ë•Œ __init__() ë©”ì†Œë“œ ì•ˆì— êµ¬ì„±ëœ ë¼ˆëŒ€ë¥¼ ë¨¼ì € ì‚´í´ë³´ë„ë¡ í•˜ì„¸ìš” :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2d3ea",
   "metadata": {},
   "source": [
    "# 15-5. Huggingface transformers (2) Tokenizer\n",
    "ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ë¬¸ì œë¥¼ í’€ ëª¨ë¸ì„ ì •í–ˆë‹¤ë©´, ì´ì œ ëª¨ë¸ì— ë„£ì„ inputì„ ë§Œë“¤ì–´ ì¤„ ì°¨ë¡€ì…ë‹ˆë‹¤.\n",
    "\n",
    "transformersëŠ” ë‹¤ì–‘í•œ tokenizerë¥¼ ê° ëª¨ë¸ì— ë§ì¶”ì–´ ì´ë¯¸ êµ¬ë¹„í•´ë‘ì—ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í•  ì¼ì€ tokenizerë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒë¿ì´ëë‹ˆë‹¤. ì‚¬ìš©í•˜ê¸°ì— ì•ì„œì„œ, ë‚´ê°€ ì„ íƒí•œ modelì´ ì–´ë– í•œ tokenizerë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ ì •ë„ ë¯¸ë¦¬ ì²´í¬í•´ë‘ëŠ” ì„¼ìŠ¤ëŠ” ëª¨ë‘ ì±™ê²¨ë‘ì…¨ê² ì£ ?:)\n",
    "\n",
    "Pretrained model ê¸°ë°˜ì˜ NLP frameworkë¥¼ ì‚¬ìš©í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ë‘ ê°€ì§€ í´ë˜ìŠ¤ëŠ” Modelê³¼ Tokenizerë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‘ ê°€ì§€ëŠ” ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° êµ¬ì¡°ê°€ ë™ì¼í•œ Modelì´ë¼ í•˜ë”ë¼ë„ Tokenizerê°€ ë‹¤ë¥´ê±°ë‚˜ Tokenizer ë‚´ì˜ Dictionaryê°€ ë‹¬ë¼ì§€ë©´ ì‚¬ì‹¤ìƒ ì™„ì „íˆ ë‹¤ë¥¸ ëª¨ë¸ì´ ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  TokenizerëŠ” ì–´ë–¤ ì–¸ì–´ë¥¼ ë‹¤ë£¨ëŠëƒ í•˜ëŠ” ì½”í¼ìŠ¤ ë°ì´í„°ì…‹ì— ë”°ë¼ì„œë„ ë‹¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì „ ìŠ¤í…ì—ì„œ ì†Œê°œí–ˆë˜ Huggingfaceê°€ ì œê³µí•˜ëŠ” ëª¨ë¸ ì¢…ë¥˜ ì¤‘ ëª‡ ê°œë§Œ ì˜ˆë¡œ ë“¤ì–´ë³¼ê¹Œìš”?\n",
    "\n",
    "- bert-base-uncased : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì„ ì—†ì•´ë‹¤(ì „ì²´ ì†Œë¬¸ìí™”)\n",
    "- bert-large-cased : BERT ëª¨ë¸ì¸ë°, 340MB íŒŒë¼ë¯¸í„°ì˜ ëŒ€í˜• ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ì˜ë¬¸ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì„ ìœ ì§€í–ˆë‹¤.\n",
    "- bert-base-multilingual-cased : BERT ëª¨ë¸ì¸ë°, 108MB íŒŒë¼ë¯¸í„°ì˜ ê¸°ë³¸ ëª¨ë¸ì´ë©´ì„œ, ì½”í¼ìŠ¤ëŠ” ë‹¤êµ­ì–´ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ìœ ì§€í–ˆë‹¤.\n",
    "\n",
    "tokenizer ë˜í•œ ì§ì ‘ ëª…ì‹œí•˜ì—¬ ë‚´ê°€ ì‚¬ìš©í•  ê²ƒì„ ì§€ì •í•´ ì£¼ê±°ë‚˜, AutoTokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ êµ¬ë¹„ëœ modelì— ì•Œë§ì€ tokenizerë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "ì´ë•Œ ìœ ì˜í•  ì ì€, modelì„ ì‚¬ìš©í•  ë•Œ ëª…ì‹œí–ˆë˜ ê²ƒê³¼ ë™ì¼í•œ IDë¡œ tokenizerë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e987476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a754eaffe0433d920d078aa4c42842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d063ebe8c545f8942629630dbef6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ba077d267e4ae9a04d1844c8bb0386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b62b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c511c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66f718",
   "metadata": {},
   "source": [
    "ì´ ê²½ìš°ëŠ” BERTì˜ tokenizerì´ê¸° ë•Œë¬¸ì— ì¸ì½”ë”©ì´ ëœ input_ids ë¿ë§Œ ì•„ë‹ˆë¼, token_type_idsì™€ attention_maskê¹Œì§€ ëª¨ë‘ ìƒì„±ëœ input ê°ì²´ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tokenizerëŠ” batch ë‹¨ìœ„ë¡œ inputì„ ë°›ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd2ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604ea74",
   "metadata": {},
   "source": [
    "ì´ ë°–ì—ë„ tokenizeí•  ë•Œì— padding, truncation ë“± ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ ì–´ë–¤ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€(Tensorflow ë˜ëŠ” PyTorch)ì— ë”°ë¼ input íƒ€ì…ì„ ë³€ê²½ ì‹œì¼œì£¼ëŠ” return_tensors ì¸ìë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c31fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]],\n",
      "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135eacd3",
   "metadata": {},
   "source": [
    "# 15-6. Huggingface transformers (3) Processor\n",
    "ì§€ê¸ˆê¹Œì§€ ê°€ì¥ í•µì‹¬ì´ ë˜ëŠ” Model ë° Modelê³¼ ë°ì´í„°ì…‹ì„ ì—°ê²°í•´ ì£¼ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì—°ê²° ë„êµ¬ì¸ Tokenizerì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ°ë°, ê³¼ì—° Tokenizerê¹Œì§€ë§Œ ìˆìœ¼ë©´ ì–´ë–¤ Taskì´ë“  Modelì— ë„£ì„ ìˆ˜ ìˆëŠ” ì ì ˆí•œ ì…ë ¥ í˜•íƒœë¡œ ë³€ê²½í•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?\n",
    "\n",
    "ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ BERTì˜ pretrainingë§Œ í•˜ë”ë¼ë„, ëª¨ë¸ì— ë“¤ì–´ê°ˆ ì…ë ¥ ë¶€ë¶„ì„ êµ¬ì„±í•˜ë ¤ë©´\n",
    "\n",
    "1. ë‘ ê°œì˜ ë¬¸ì¥ì„ ê³¨ë¼ì„œ, Next Sentence Predictionì„ ìœ„í•´ ì ì ˆíˆ ë°°ì¹˜í•˜ê³ ,\n",
    "2. 15%ì˜ ë§ˆìŠ¤í‚¹ í¬ì§€ì…˜ì„ ê³¨ë¼ë‚´ê¸° ìœ„í•œ ë³µì¡í•œ ê³¼ì •ì„ ê±°ì¹œ í›„, ì‹¬ì§€ì–´ ê·¸ ë§ˆí‚¹ ëŒ€ìƒë„ 10%, 10%ì˜ í™•ë¥ ë¡œ ë§ˆìŠ¤í‚¹ ëŒ€ì‹  ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ í•´ì£¼ì–´ì•¼ í•˜ê³ ,\n",
    "3. Next Sentence Predictionì„ ìœ„í•´ Segment Embeddingì„ ìœ„í•œ tensorë¥¼ ë”°ë¡œ ë§ˆë ¨í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë¶€ë¶„ì˜ ê³¼ì •ì´ ì˜ì™¸ë¡œ (ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒë§Œí¼ì´ë‚˜) ë³µì¡í•œ ê³¼ì •ì„ì„ ê²½í—˜í•´ ë³´ì…¨ê² ì£ ?\n",
    "\n",
    "ì´ë ‡ê²Œ ë³µì¡í•œ ê³¼ì •ì„ ìœ„í•´ì„œëŠ” Tokenizerë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ Huggingfaceì—ì„œëŠ” Processorë¼ëŠ” ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” Processor ì¤‘ Sequence Classification íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤ì¸ DataProcessorì˜ ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885bcd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"sequence classificationì„ ìœ„í•´ dataë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë³¸ processor\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        tensor dictì—ì„œ exampleì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì†Œë“œ\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"train dataì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"dev data(validation data)ì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"test dataì—ì„œ InputExample í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê²ƒë“¤ì„ ëª¨ìœ¼ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"data setì— ì‚¬ìš©ë˜ëŠ” ë¼ë²¨ë“¤ì„ ë¦¬í„´í•˜ëŠ” ë©”ì†Œë“œ\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        tfds(tensorflow-datasets)ì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ DataProcessorì— ì•Œë§ê²Œ ê°€ê³µí•´ì£¼ëŠ” ë©”ì†Œë“œ\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"tabìœ¼ë¡œ êµ¬ë¶„ëœ .tsvíŒŒì¼ì„ ì½ì–´ë“¤ì´ëŠ” í´ë˜ìŠ¤ ë©”ì†Œë“œ\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822e87c",
   "metadata": {},
   "source": [
    "processorëŠ” raw dataë¥¼ ê°€ê³µí•˜ì—¬ modelì— íƒœìš¸ ìˆ˜ ìˆëŠ” í˜•íƒœë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ì‘ì—…ì„ í•´ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "Hugging faceëŠ” SQuAD, GLUE ë“± ê°€ì¥ ëŒ€í‘œì ì¸ NLPì˜ ë¬¸ì œë“¤ì— ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ processorë¥¼ ë§Œë“¤ì–´ ë‘ì—ˆìŠµë‹ˆë‹¤. ë§Œì•½, ë‚´ê°€ ì§ì ‘ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ê³  ì‹¶ë‹¤ë©´, ë‚´ ë°ì´í„°ì— ì•Œë§ì€ processorë¥¼ ì§ì ‘ ì •ì˜í•´ì•¼ê² ì£ ?\n",
    "\n",
    "Task ë³„ ë³µì¡í•œ ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” processorë¥¼ ì§ì ‘ ë§Œë“œì‹¤ ë•ŒëŠ”, DataProcessorë¥¼ ìƒì†ë°›ì•„ì„œ ë§Œë“¤ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ì— ë”°ë¼ì„œ ì¶”ê°€í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ ìƒê¸¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í”„ë¡œì íŠ¸ ë…¸ë“œì—ì„œëŠ” processorë¥¼ ìƒì†ë°›ì•„ í™œìš©í•˜ëŠ” ë¶€ë¶„ë„ ì‹¤ìŠµì„ í†µí•´ ì§„í–‰í•´ ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42cf4b",
   "metadata": {},
   "source": [
    ">>â— ì£¼ì˜\n",
    "raise NotImplementedError()ëŠ” ì§€ì›Œì£¼ì‹œëŠ” ê²ƒ ìŠì§€ ë§ˆì„¸ìš”! ì¶”ìƒ í´ë˜ìŠ¤ì—ì„œ ê¼­ êµ¬í˜„í•´ì•¼ í•  ë¶€ë¶„ì´ NotImplementedë¡œ ë‚¨ì•„ìˆë‹¤ë©´ ì—¬ëŸ¬ë¶„ì´ ì§  ProcessorëŠ” Tokenizerì™€ í˜‘ë ¥í•˜ì—¬ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6e83c",
   "metadata": {},
   "source": [
    "# 15-7. Huggingface transformers (4) Config\n",
    "configëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ìš”ì†Œë“¤ì„ ëª…ì‹œí•œ jsoníŒŒì¼ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ jsoníŒŒì¼ì—ëŠ” batch size, learning rate, weight_decayë“± trainì— í•„ìš”í•œ ìš”ì†Œë“¤ë¶€í„° tokenizerì— íŠ¹ìˆ˜ í† í°(special token eg.[MASK])ë“¤ì„ ë¯¸ë¦¬ ì„¤ì •í•˜ëŠ” ë“± ì„¤ì •ì— ê´€í•œ ì „ë°˜ì ì¸ ê²ƒë“¤ì´ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "PretrainedModelì„ save_pretrained ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ë©´ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ì €ì¥ë˜ë„ë¡ ë˜ì–´ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "hugging faceì˜ pretrained modelì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìë™ìœ¼ë¡œ configíŒŒì¼ì´ ë¡œë“œë˜ì–´ ëª…ì‹œí•  í•„ìš”ê°€ ì—†ì§€ë§Œ, ì„¤ì •ì„ ë³€ê²½í•˜ê³  ì‹¶ê±°ë‚˜ ë‚˜ë§Œì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œì—ëŠ” configíŒŒì¼ì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "config ë˜í•œ model, tokenizerì²˜ëŸ¼ Model IDë§Œ ìˆìœ¼ë©´, Config í´ë˜ìŠ¤ë¥¼ ëª…í™•íˆ ì§€ì •í•˜ê±°ë‚˜ í˜¹ì€ AutoConfigë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b681aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf38732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdf8dd",
   "metadata": {},
   "source": [
    "ë‘ ë°©ì‹ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¨ configì˜ ë‚´ìš©ì— ë³„ë‹¤ë¥¸ ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ëª¨ë¸ì„ ì´ë¯¸ ìƒì„±í–ˆë‹¤ë©´ model.configìœ¼ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4853961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "config = model.config\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12395d8",
   "metadata": {},
   "source": [
    "# 15-8. Huggingface transformers (5) Trainer\n",
    "trainerëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. training, fine-tuning, evaluation ëª¨ë‘ trainer classë¥¼ ì´ìš©í•˜ì—¬ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tensorflowì˜ ê²½ìš° tf.keras.model APIë¥¼ ì´ìš©í•˜ì—¬ì„œë„ Huggingfaceë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ í™œìš©í•´ í•™ìŠµì´ë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë™ì•ˆ ë§ì´ í™œìš©í•´ ë³´ì•˜ë˜ model.fit()ì´ë‚˜ model.predict()ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ë§Œ, TFTrainerë¥¼ ì´ìš©í•  ê²½ìš°ì—ëŠ” TrainingArguments ë¥¼ í†µí•´ Huggingface í”„ë ˆì„ì›Œí¬ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ë“¤ì„ í†µí•©ì ìœ¼ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì½”ë“œëŠ” Huggingfaceë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì„ tf.keras.model APIë¥¼ ì´ìš©í•´ í™œìš©í•˜ëŠ” ê²½ìš°ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8543a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Results=====\n",
      "TFBertForPreTrainingOutput(loss=None, prediction_logits=array([[[ -7.402721 ,  -7.362659 ,  -7.4500127, ...,  -6.1955233,\n",
      "          -5.894807 ,  -6.3672695],\n",
      "        [ -7.828724 ,  -8.0582285,  -7.864206 , ...,  -6.419408 ,\n",
      "          -6.3024364,  -6.7624674],\n",
      "        [-11.549931 , -11.551905 , -11.484696 , ...,  -8.114805 ,\n",
      "          -8.314197 ,  -9.4444475],\n",
      "        ...,\n",
      "        [ -3.2660656,  -3.7416406,  -2.5797932, ...,  -4.0109997,\n",
      "          -2.4964375,  -3.0753887],\n",
      "        [-12.231965 , -12.027048 , -11.797831 , ...,  -8.838844 ,\n",
      "          -9.09165  , -10.497256 ],\n",
      "        [-10.639945 , -11.074341 , -11.0361   , ...,  -8.148463 ,\n",
      "          -9.585199 , -10.67151  ]]], dtype=float32), seq_relationship_logits=array([[ 1.6309196 , -0.71684647]], dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForPreTraining, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "sentence = \"Hello, This is test for bert TFmodel.\"\n",
    "\n",
    "input_ids = tf.constant(tokenizer.encode(sentence, add_special_tokens=True))[None, :]  # Batch size 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "pred = model.predict(input_ids)\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5362aeb",
   "metadata": {},
   "source": [
    "TFTrainerë¥¼ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” í•™ìŠµì— í•„ìš”í•œ argumentsì„ TFTrainingArgumentsì„ í†µí•´ì„œ ì •ì˜í•´ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” TFTrainerë¥¼ ì‚¬ìš©í•˜ì—¬ Huggingface ëª¨ë¸ì˜ í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œë¡œ ëª¨ë¸ì´ êµ¬ë™ë˜ì–´ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•œ í”„ë¡œì íŠ¸ êµ¬ì„±ì€ ë‹¤ìŒ í”„ë¡œì íŠ¸ ë…¸ë“œì—ì„œ ë‹¤ë£¨ê² ì§€ë§Œ, ì´ë²ˆ ë…¸ë“œì—ì„œ ì‚´í´ë³¸ Model, Tokenizer ë° ë°ì´í„°ì…‹ êµ¬ì„±ì´ TFTrainingArgumentsë¥¼ í†µí•´ì„œ TFTrainerì— ì–´ë–»ê²Œ ë°˜ì˜ë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e62189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89063bc99a79499089a48c17394eec80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e659374655dd43d284f0383ae0a9c764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbecafbddb048f7b1ca094c257aa951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecfa277d1e748ee82e87eb18fb4b7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3458800b7e6d4540970bd91dc2e8be07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict, Optional\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    glue_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "# TFTrainingArguments ì •ì˜\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',              # outputì´ ì €ì¥ë  ê²½ë¡œ\n",
    "    num_train_epochs=1,              # train ì‹œí‚¬ ì´ epochs\n",
    "    per_device_train_batch_size=16,  # ê° device ë‹¹ batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation ì‹œì— batch size\n",
    "    warmup_steps=500,                # learning rate schedulerì— ë”°ë¥¸ warmup_step ì„¤ì •\n",
    "    weight_decay=0.01,                 # weight decay\n",
    "    logging_dir='./logs',                 # logê°€ ì €ì¥ë  ê²½ë¡œ\n",
    "    do_train=True,                        # train ìˆ˜í–‰ì—¬ë¶€\n",
    "    do_eval=True,                        # eval ìˆ˜í–‰ì—¬ë¶€\n",
    "    eval_steps=1000\n",
    ")\n",
    "\n",
    "# model, tokenizer ìƒì„±\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "with training_args.strategy.scope():    # training_argsê°€ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” modelì˜ ë²”ìœ„ë¥¼ ì§€ì •\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_pt=bool(\".bin\" in model_name_or_path),\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e40c61",
   "metadata": {},
   "source": [
    "ìœ„ì™€ ê°™ì´ Huggingface í”„ë ˆì„ì›Œí¬ êµ¬ì¡°ì— ë”°ë¼ Modelê³¼ Tokenizerë¥¼ ê°„ë‹¨íˆ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ëŠ” ê¸°ì¡´ì— ì‚´í´ë³¸ ê²ƒê³¼ í° ì°¨ì´ëŠ” ì—†ì§€ë§Œ model ìƒì„± ì‹œì— training_argsì˜ scope ì•ˆì—ì„œ ì§„í–‰í–ˆë‹¤ëŠ” ê²ƒì´ ëˆˆì— ë•ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ TFTrainer ì‚¬ìš© ì‹œ ê²°ì •ì ìœ¼ë¡œ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ with êµ¬ë¬¸ì„ ìƒëµí•˜ë©´ TFTrainerì— ì „ë‹¬í•˜ê³ í”ˆ ì˜µì…˜ì´ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•„ ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì´ ì˜¤ë™ì‘í•˜ê²Œ ë˜ëŠ” ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6001ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: glue/mrpc/2.0.0\n",
      "INFO:absl:Load dataset info from /tmp/tmpq_v4t1v7tfds\n",
      "INFO:absl:Generating dataset glue (/aiffel/tensorflow_datasets/glue/mrpc/2.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.43 MiB (download: 1.43 MiB, generated: 1.74 MiB, total: 3.17 MiB) to /aiffel/tensorflow_datasets/glue/mrpc/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cea780980145d5b18f17186b4dda19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6427a2441ee14853a4774dddc5b8ddf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc into /aiffel/tensorflow_datasets/downloads/fire.goog.com_v0_b_mtl-sent-repr.apps.com_o_2FjSIMlCiqs1QSmIykr4IRPnEHjPuGwAz5i40v8K9U0Z8.tsvalt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc.tmp.4eafdcfb44884ddf852811c171fac535...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt into /aiffel/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_test0PdekMcyqYR-w4Rx_d7OTryq0J3RlYRn4rAMajy9Mak.txt.tmp.5f27cc0a7a1648d6958eb1881a404d61...\n",
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt into /aiffel/tensorflow_datasets/downloads/dl.fbaip.com_sente_sente_msr_parap_trainfGxPZuQWGBti4Tbd1YNOwQr-OqxPejJ7gcp0Al6mlSk.txt.tmp.0ceff0a4631a4478a7f13ae9a055a6e0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-train.tfrecord...:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-train.tfrecord. Number of examples: 3668 (shards: [3668])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-validation.tfrecord...:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-validation.tfrecord. Number of examples: 408 (shards: [408])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling glue-test.tfrecord...:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing glue-test.tfrecord. Number of examples: 1725 (shards: [1725])\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /aiffel/tensorflow_datasets/glue/mrpc/2.0.0\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/data/processors/glue.py:175: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /aiffel/tensorflow_datasets/glue/mrpc/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer_tf.py:109: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4654414918687608}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "ds, info = tfds.load('glue/mrpc', with_info=True)\n",
    "train_dataset = glue_convert_examples_to_features(ds['train'], tokenizer, 128, 'mrpc')\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['train'].num_examples))\n",
    "\n",
    "# TFTrainer ìƒì„±\n",
    "trainer = TFTrainer(\n",
    "    model=model,                          # í•™ìŠµì‹œí‚¬ model\n",
    "    args=training_args,                  # TFTrainingArgumentsì„ í†µí•´ ì„¤ì •í•œ arguments\n",
    "    train_dataset=train_dataset,   # training dataset\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ì§„í–‰\n",
    "trainer.train()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_dataset = glue_convert_examples_to_features(ds['test'], tokenizer, 128, 'mrpc')\n",
    "test_dataset = test_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['test'].num_examples))\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601078a",
   "metadata": {},
   "source": [
    "ì´í›„ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ì—¬, model, training_argsê³¼ í•¨ê»˜ TFTrainerì— ì „ë‹¬í•˜ëŠ” ê²ƒìœ¼ë¡œ í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„ê°€ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤. ì´í›„ trainer.train()ì„ í˜¸ì¶œí•˜ë©´ ì‹¤ì œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16539a8c",
   "metadata": {},
   "source": [
    "# 15-9. ë§ˆë¬´ë¦¬í•˜ë©°\n",
    "ì´ë ‡ê²Œ Huggingfaceê°€ ë§Œë“  frameworkì¸ transformersë¥¼ í›‘ì–´ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‚´ê°€ ì–´ë–»ê²Œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ê³ , ë°ì´í„°ë¥¼ ê°€ê³µí•´ì•¼ í•˜ëŠ”êµ¬ë‚˜ ì¡°ê¸ˆ ê°ì´ ì¡íˆì…¨ì„ê¹Œìš”? ë¹„ë‹¨ transformersë¿ë§Œ ì•„ë‹ˆë¼, ë‹¤ë¥¸ frameworkë¥¼ ì‚¬ìš©í•˜ì‹¤ ë•Œì—ë„ ì´ì²˜ëŸ¼ frameworkì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ë¥¼ ë¨¼ì € íŒŒì•…í•˜ì‹ ë‹¤ë©´, ë”ìš± ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆì„ ê±°ë¼ê³  ìì‹  ìˆê²Œ ë§ì”€ë“œë¦½ë‹ˆë‹¤!!!!!!\n",
    "\n",
    "ì´ì œ transformersë¥¼ ì´ìš©í•˜ì—¬ ì´ì „ ì‹œê°„ì— ë°°ì› ë˜ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ê³¼ íŠ¹ì„±ì„ ë¹„êµí•´ ë³´ê¸°ë„ í•˜ê³ , ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ë„ ìˆê² ì£ ?\n",
    "\n",
    "ë”.ë‚˜.ì•„.ê°€.\n",
    "\n",
    "ë‚˜ë§Œì˜ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í’€ì–´ë³´ê¸°ë„ í•˜ë©´ ì™„-ë²½-â˜† í•˜ê²Œ NLPì˜ ë§ˆìŠ¤í„°ê°€ ë˜ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”ğŸ¤—\n",
    "ë‹¤ìŒ ì‹œê°„ì— ì§„í–‰í•  ë‚´ìš©ì„ ê¸°ëŒ€í•´ ì£¼ì„¸ìš”~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
